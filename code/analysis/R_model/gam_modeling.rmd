---
title: "GAM model building"
author: "Darya Akimova"
date: "9/23/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Setup

## Packages:

```{r packages, comment=NA}
library(tidyverse)
library(cowplot)
library(viridis)
#library(sf)
library(mgcv)
theme_set(theme_minimal())
```


## Data:

```{r data_import_old, comment=NA, eval=FALSE}
ma_town_coord <- read_csv("../../../data/tidy_data/ma_town_crs4326_coords.csv")
#ma_town_map <- st_read("../../../data/raw_data/townssurvey_shp/TOWNSSURVEY_POLYM.shp")
acs_medicare_opioid_merge <- read_csv("../../../data/tidy_data/acs_medicare_opioid_stats_death_rate.csv")
death_acs_merge_full <- read_csv("../../../data/tidy_data/death_count_norm_to_pop_and_acs_town_demographics_merge_all_cols.csv")
benzo_pres <- read_csv("../../../data/tidy_data/med_partD_benzo_sum_w_town_merge_13_to_17.csv")
```


```{r}
ma_town_coord <- read_csv("../../../data/tidy_data/ma_town_crs4326_coords.csv")
benzo_pres <- read_csv("../../../data/tidy_data/med_partD_benzo_sum_w_town_merge_13_to_17.csv")
death_acs_opi_pres <- read_csv("../../../data/tidy_data/acs_medicare_opioid_stats_death_count_merge.csv")
```



# Dataset prep


```{r}
death_acs_opi_coord <- death_acs_opi_pres %>% 
  mutate(id = row_number()) %>% 
  inner_join(ma_town_coord, by = c("city_death" = "town"))
benzo_pres %>% 
  ggplot(aes(sqrt(total_claim_count), sqrt(total_day_supply), color = generic_name)) +
  geom_point(alpha = 0.5, size = 1) +
  facet_wrap(~ year)
benzo_pres %>% 
  ggplot(aes(sqrt(total_claim_count), sqrt(total_30_day_fill_count), color = generic_name)) +
  geom_point(alpha = 0.5, size = 1) +
  facet_wrap(~ year)
benzo_tot_day_supp <- benzo_pres %>% 
  select(town:year, total_day_supply) %>% 
  spread(key = generic_name, value = total_day_supply)
benzo_tot_day_supp %>% 
  ggplot(aes(sqrt(alprazolam), sqrt(diazepam))) +
  geom_point() +
  facet_wrap(~ year)
benzo_tot_day_supp %>% 
  ggplot(aes(sqrt(alprazolam), sqrt(lorazepam))) +
  geom_point() +
  facet_wrap(~ year)
sapply(data.frame(sapply(benzo_tot_day_supp, is.na)), sum)
benzo_tot_day_supp_nona <- benzo_tot_day_supp %>% 
  replace(is.na(.), 0) %>% 
  rename(city_death = town)
sapply(data.frame(sapply(benzo_tot_day_supp_nona, is.na)), sum)
benzo_tot_day_supp_nona %>% 
  ggplot(aes(sqrt(alprazolam), sqrt(diazepam))) +
  geom_point() +
  facet_wrap(~ year)
benzo_tot_day_supp_nona
mod_df_count <- death_acs_opi_coord %>% 
  inner_join(benzo_tot_day_supp_nona, by = c("city_death", "year")) %>% 
  mutate(
    alprazolam_per_65_and_over = alprazolam / over_65_count,
    diazepam_per_65_and_over = diazepam / over_65_count,
    lorazepam_per_65_and_over = lorazepam / over_65_count,
    tot_benzo_count = alprazolam + diazepam + lorazepam,
    tot_benzo_per_65_and_over = tot_benzo_count / over_65_count
  ) %>% 
  select(-c(alprazolam, diazepam, lorazepam, town))
colnames(mod_df_count)
summary(mod_df_count)
sapply(data.frame(sapply(mod_df_count, is.na)), sum)
```


# Modeling


## Data Split

Here, the years refer to the feature years (the outcome variable is actually the 2018 opioid overdose death count).

Train set: 2014-2016(2013-2015 below)
Validation set: 2017 (2016 below)
Test set (for final model): 2018 (2017 below)


```{r data_split}
test_set <- mod_df_count %>% 
  filter(year == 2017)
valid_set <- mod_df_count %>% 
  filter(year == 2016)
train_set <- mod_df_count %>% 
  filter(year < 2016)
train_set %>% 
  count(year)
mean(mod_df_count$death_count_next_year)
var(mod_df_count$death_count_next_year)
```


Note: Variance =/= Mean - data is overdispersed and a poisson is not appropriate - try negative binomial distribution.

## Functions

Note: Concurvity is a metric provided by mgcv to evaluate the relationship between features (generalization of linearity) - ranges from 0 (good) to 1 (bad - strong relationship with other features). Concurvity can be evaluated for a feature in relation to all others (this is provided by the call `concurvity(model, full = TRUE)`), and I will focus on the "worst-case" concurvity metric. According to the Noam Ross course on GAMs (datacamp), 0.84 is a good rule of thumb for worst-case concurvity tolerance. Concurvity can also be evaluated between individual pairs of features (with the call `concurvity(model, full = FALSE)`) - again will focus on worst-case concurvity estimate.

Simple functions to calculate RMSE error to evaluate the model performace and a function to tidy up the concurvity dataframe output by mgcv (converts to a dataframe and select out the "worst" case concurvity):

```{r functions}
# rmse error calc to evaluate models
rmse <- function(model, df) {
  rmse_error <- (df$death_count_next_year - predict(model, df, type = "response")) ^ 2 %>% 
    mean() %>% 
    sqrt()
  return(rmse_error)
}
# concurvity transform to pull out the worst-case relationships between features
concurvity_df <- function(model) {
  worst_conc_tbl <- concurvity(model, full = FALSE) %>%
    data.frame() %>% 
    rownames_to_column("feature") %>% 
    tbl_df() %>% 
    mutate_if(is.numeric, round, 3) %>% 
    select(feature, contains("worst"))
  return(worst_conc_tbl)
}
```


## Model building - feature selection and parameter tuning

Base model - only the required variables of:
* geospatial coordinates, incorporated as an interaction term s(x, y) - centroids of the municipalities left in the dataset
* population to normalize opioid overdose death counts
* year

Note: `method="REML"` is a method to select the optimal smoothing parameter (to prevent overfitting and underfitting) - controls how close to the data points each individual smooth will get.

```{r base_model}
base_model <- gam(
  death_count_next_year ~ s(x,y) + s(year, k = 3) + s(tot_pop),
  data = train_set, method = "REML", family = nb()
  )
summary(base_model)
gam.check(base_model)
concurvity(base_model)
qq.gam(base_model, cex = 4)
# error on training
rmse(base_model, train_set)
# error on validation
rmse(base_model, valid_set)

mean(train_set$death_count_next_year)
sd(train_set$death_count_next_year)
mean(valid_set$death_count_next_year)
sd(valid_set$death_count_next_year)
```

Error on training data slightly smaller than the mean, but error slightly larger than the mean for the validations set. Both smaller than the standard deviation.


Full model to get a sense of other extreme (will likely overfit the data):


```{r full_model}
### all features
full_model <- gam(
    death_count_next_year ~ s(x,y) + s(year, k=3) + s(tot_pop) + s(med_house_inc) + s(mean_house_inc) + s(at_or_below_pov_prop) + s(pop_struggling_prop) + town_status + urb_v_rur + s(opioid_rate_avg) + s(less_than_hs_ed) + s(alprazolam_per_65_and_over) + s(diazepam_per_65_and_over) + s(lorazepam_per_65_and_over) + s(tot_benzo_per_65_and_over),
    data = train_set, method = "REML", family = nb(), select = TRUE
    )
summary(full_model)
gam.check(full_model)
concurvity(full_model)
qq.gam(full_model, cex = 4)
# error on training
rmse(full_model, train_set)
# error on validation
rmse(full_model, valid_set)
```


Notes:

GAM Summary:
* tot_benzo_per_65_and_over - shrunken coefficient and not significant
* diazepam_per_65_and_over - not significant
* urb_v_rururban  - not significant

GAM check:
* many features have significant p-value - indicator that more basis functions may be needed
* will keep running gam check, but will largely ignore the basis-function stat test until the list of features in the model is finalized

Concurvity:
* Concurvity very high (0.9-1 for most features) - definitely need to remove features

RMSE Error:
* both train and validation error lower than the base model, but that's not too surprising


Concurvity check on pairs of individual features:

```{r full_model_concurvity}
full_model_conc <- concurvity_df(full_model)
# result:
full_model_conc
# heatmap plot
full_model_conc <- full_model_conc %>% 
  filter(feature != "para") %>% 
  select(-worst.para) %>% 
  gather(key="feature2", value="value", -feature) %>% 
  ggplot(aes(feature, feature2, fill=value)) +
  geom_raster() +
  scale_fill_viridis_c() +
  theme(axis.text.x = element_text(angle=45, hjust=1)) +
  ggtitle("Concurvity Plot\nFull Model") +
  xlab("Feature 1") +
  ylab("Feature 2")
full_model_conc
# save for blog:
#save_plot(filename = "../../../figures/tidy_figures/full_model_concurvity.png", full_model_conc, base_height = 6, base_width = 8)
```

Concurvity 0 (best) - 1 (worst)

* diazepam and tot_benzo were not predictive and also have very high concurvity with several features - good candidates for removal
* median and mean income are basically the same feature - will have to pick one


Q: Does it matter if the data is scaled? Should not because a GAM is a nonlinear type of modeling, but worth it to check.


```{r full_model_scaled}
# scale training data:
train_set_scaled <- train_set %>% 
  select(-c(year:death_count_next_year, x:y)) %>% 
  mutate_if(is.numeric, scale) %>% 
    bind_cols(
      train_set %>% 
        select(year:death_count_next_year, x:y)
    )
colnames(train_set_scaled)
full_model_scaled <- gam(
    death_count_next_year ~ s(x,y) + s(year, k=3) + s(tot_pop) + s(med_house_inc) + s(mean_house_inc) + s(at_or_below_pov_prop) + s(pop_struggling_prop) + town_status + urb_v_rur + s(opioid_rate_avg) + s(less_than_hs_ed) + s(alprazolam_per_65_and_over) + s(diazepam_per_65_and_over) + s(lorazepam_per_65_and_over) + s(tot_benzo_per_65_and_over),
    data = train_set_scaled, method = "REML", family = nb(), select = TRUE
    )
summary(full_model_scaled)
summary(full_model)
# error on training
rmse(full_model_scaled, train_set_scaled)
```


Scaling and not scaling outputs the same model - good. Don't need to scale, makes for a simpler workflow.


Now start removing features. For this version, remove:
* med_house_inc
* tot_benzo_per_65_and_over

```{r full_sub1}
full_model_sub1 <- gam(
    death_count_next_year ~ s(x,y) + s(year, k=3) + s(tot_pop) + s(mean_house_inc) + s(at_or_below_pov_prop) + s(pop_struggling_prop) + town_status + urb_v_rur + s(opioid_rate_avg) + s(less_than_hs_ed) + s(alprazolam_per_65_and_over) + s(diazepam_per_65_and_over) + s(lorazepam_per_65_and_over),
    data = train_set, method = "REML", family = nb(), select = TRUE
    )
summary(full_model_sub1)
gam.check(full_model_sub1)
concurvity(full_model_sub1)
qq.gam(full_model_sub1, cex = 4)
# error on training
rmse(full_model_sub1, train_set)
# error on validation
rmse(full_model_sub1, valid_set)
```


Notes:

GAM Summary:
* now at_or_below_pov_prop no longer significant
* diazepam_per_65_and_over and urb_v_rururban still not significant

GAM check:
* still ignoring this mostly 

Concurvity:
* Concurvity still high, but most have gone down at least a little bit

RMSE Error:
* train error higher
* validation error slightly lower (may not be meaningful), could be sign I'm not overfitting as much as full model


Individual feature concurvity check:

```{r full_sub1_concurvity}
full_model_sub1_conc <- concurvity_df(full_model_sub1)
full_model_sub1_conc %>% 
  gather(key="feature2", value="value", -feature) %>% 
  ggplot(aes(feature, feature2, fill=value)) +
  geom_raster() +
  scale_fill_viridis_c() +
  theme(axis.text.x = element_text(angle=45, hjust=1)) +
  ggtitle("Concurvity plot\nFull Model - sub 1")
full_model_sub1_conc %>% 
  select(feature, contains("per_65_and_over"))
```


Diazepam is not very predictive (p-val not sig) and has high concurvity with more predictive features - remove.


Sub 2 - take out diazepam term:

```{r full_sub2}
full_model_sub2 <- gam(
    death_count_next_year ~ s(x,y) + s(year, k=3) + s(tot_pop) + s(mean_house_inc) + s(at_or_below_pov_prop) + s(pop_struggling_prop) + town_status + urb_v_rur + s(opioid_rate_avg) + s(less_than_hs_ed) + s(alprazolam_per_65_and_over) + s(lorazepam_per_65_and_over),
    data = train_set, method = "REML", family = nb(), select = TRUE
    )
summary(full_model_sub2)
gam.check(full_model_sub2)
concurvity(full_model_sub2)
qq.gam(full_model_sub2, cex = 4)
# error on training
rmse(full_model_sub2, train_set)
# error on validation
rmse(full_model_sub2, valid_set)
```


Notes:

GAM Summary:
* at_or_below_pov_prop and urb_v_rururban still not significant

GAM check:
* still ignoring this mostly 

Concurvity:
* lorazepam and alprazolam concurvities below/close to "removal" threshold of 0.84 now

RMSE Error:
* training error about the same as previous iteration (sub1)
* validation error slightly less than previous iteration (sub1) - but almost the same


Individual feature concurvity check:

* the 2 poverty metrics (at_or_below_pov_prop, pop_struggling_prop) are good candidates for removing one of them to reduce concurvity further

```{r full_sub2_concurvity}
full_model_sub2_conc <- concurvity_df(full_model_sub2)
full_model_sub2_conc %>% 
  select(feature, contains("prop")) %>% 
  arrange(desc(worst.s.at_or_below_pov_prop.))
# sum of these individual concurvity values:
full_model_sub2_conc %>% 
  select(contains("prop")) %>% 
  summarize_all(sum)
```

`at_or_below_pov_prop` has a slightly higher total concurvity with the other features in the model - try removing it first. 


```{r full_sub3}
full_model_sub3 <- gam(
    death_count_next_year ~ s(x,y) + s(year, k=3) + s(tot_pop) + s(mean_house_inc) + s(pop_struggling_prop) + town_status + urb_v_rur + s(opioid_rate_avg) + s(less_than_hs_ed) + s(alprazolam_per_65_and_over) + s(lorazepam_per_65_and_over),
    data = train_set, method = "REML", family = nb(), select = TRUE
    )
summary(full_model_sub3)
gam.check(full_model_sub3)
concurvity(full_model_sub3)
qq.gam(full_model_sub3, cex = 4)
# error on training
rmse(full_model_sub3, train_set)
# error on validation
rmse(full_model_sub3, valid_set)
```

Notes:

GAM Summary:
* urb_v_rururban not significant

GAM check:
* still ignoring this mostly - alprazolam_per_65_and_over consistently significant, may need more basis functions?

Concurvity:
* will continue to ignore total population, needs to be in the model
* pop_struggling_prop, mean_house_inc, less_than_hs_ed still high

RMSE Error:
* error on both training and validation are both slightly up, but that feature had to be removed because of high concurvity


Individual feature concurvity check:

* what does total population have high concurvity with?

```{r full_sub3_concurvity}
full_model_sub3_conc <- concurvity_df(full_model_sub3)
full_model_sub3_conc %>% 
  select(feature, contains("tot_pop")) %>% 
  arrange(desc(worst.s.tot_pop.))
```


Sub4 - remove urban v rural - not informative/predictive

```{r full_sub4}
full_model_sub4 <- gam(
    death_count_next_year ~ s(x,y) + s(year, k=3) + s(tot_pop) + s(mean_house_inc) + s(pop_struggling_prop) + town_status + s(opioid_rate_avg) + s(less_than_hs_ed) + s(alprazolam_per_65_and_over) + s(lorazepam_per_65_and_over),
    data = train_set, method = "REML", family = nb(), select = TRUE
    )
summary(full_model_sub4)
gam.check(full_model_sub4)
concurvity(full_model_sub4)
qq.gam(full_model_sub4, cex = 4)
# error on training
rmse(full_model_sub4, train_set)
# error on validation
rmse(full_model_sub4, valid_set)
```

Notes:

GAM Summary:
* all features statistically significant except alprazolam_per_65_and_over, will leave it in until I deal with all the other concurvity issues

GAM check:
* still mostly ignoring this

Concurvity:
* mean_house_inc, pop_struggling_prop, less_than_hs_ed still have overall worst-case concurvity estimates over the desired threshold of 0.84

RMSE Error:
* train error slightly up
* but validation error went down by quite a bit (relatively speaking)


Individual feature concurvity check:

```{r full_sub4_concurvity}
full_model_sub4_conc <- concurvity_df(full_model_sub4)
full_model_sub4_conc %>% 
  gather(key="feature2", value="value", -feature) %>% 
  ggplot(aes(feature, feature2, fill=value)) +
  geom_raster() +
  scale_fill_viridis_c() +
  theme(axis.text.x = element_text(angle=45, hjust=1)) +
  ggtitle("Concurvity plot\nFull Model - sub 4")
```


* mean_house_inc, pop_struggling_prop, less_than_hs_ed seem to have relatively high concurvity with each other


Before moving on, try out another type of interaction term between x, y, and total population

```{r full_sub5}
full_model_sub5 <- gam(
    death_count_next_year ~ s(x,y) + s(year, k=3) + s(tot_pop) + s(mean_house_inc) + s(pop_struggling_prop) + town_status + s(opioid_rate_avg) + s(less_than_hs_ed) + s(alprazolam_per_65_and_over) + s(lorazepam_per_65_and_over) + ti(x, y, tot_pop, d=c(2, 1)),
    data = train_set, method = "REML", family = nb(), select = TRUE
    )
summary(full_model_sub5)
gam.check(full_model_sub5)
concurvity(full_model_sub5)
qq.gam(full_model_sub5, cex = 4)
# error on training
rmse(full_model_sub5, train_set)
# error on validation
rmse(full_model_sub5, valid_set)
```


Notes:

* Concurvities overall went way up again
* Error on validation went way up again too


Back pedal - take that out and go back to sub 4 - look at interaction between terms mean_house_inc, pop_struggling_prop, and less_than_hs_ed 


```{r full_sub4_concurvity_followup}
# mean income concurvity
full_model_sub4_conc %>% 
  select(feature, contains("mean")) %>% 
  arrange(desc(worst.s.mean_house_inc.))
# hs education concurvity
full_model_sub4_conc %>% 
  select(feature, contains("hs")) %>% 
  arrange(desc(worst.s.less_than_hs_ed.))
train_set %>% 
  ggplot(aes(mean_house_inc, pop_struggling_prop)) +
  geom_point(size = 2, alpha = 0.5) +
  ggtitle("Relationship between mean house income\nand proportion of population struggling (poverty measure)\nTraining set")
train_set %>% 
  ggplot(aes(mean_house_inc, less_than_hs_ed)) +
  geom_point(size = 2, alpha = 0.5) +
  ggtitle("Relationship between mean house income\nand proportion with less than HS education\nTraining set")
train_set %>% 
  ggplot(aes(pop_struggling_prop, less_than_hs_ed)) +
  geom_point(size = 2, alpha = 0.5) +
  ggtitle("Relationship between proportion of population struggling (poverty measure)\nand proportion with less than HS education\nTraining set")
full_model_sub4_conc %>% 
  select(feature, contains("tot_pop")) %>% 
  arrange(desc(worst.s.tot_pop.))
```


`less_than_hs_ed` has high concurvity with mean income and total population - try removing it


```{r full_sub6}
full_model_sub6 <- gam(
    death_count_next_year ~ s(x,y) + s(year, k=3) + s(tot_pop) + s(mean_house_inc) + s(pop_struggling_prop) + town_status + s(opioid_rate_avg) + s(alprazolam_per_65_and_over) + s(lorazepam_per_65_and_over),
    data = train_set, method = "REML", family = nb(), select = TRUE
    )
summary(full_model_sub6)
gam.check(full_model_sub6)
concurvity(full_model_sub6)
qq.gam(full_model_sub6, cex = 4)
# error on training
rmse(full_model_sub6, train_set)
# error on validation
rmse(full_model_sub6, valid_set)
```


Notes:

GAM Summary:
* alprazolam_per_65_and_over not significant

GAM check:
* maybe it's time to try upping the k for some of these features?

Concurvity:
* `mean_house_inc` and `pop_struggling_prop` still have high worst-case concurvity

RMSE Error:
* train error slightly up from previous iteration (sub4)
* validation error slight down from previous iteration (sub4)


Individual feature concurvity check:


```{r full_sub6_concurvity}
full_model_sub6_conc <- concurvity_df(full_model_sub6)
full_model_sub6_conc %>% 
  gather(key="feature2", value="value", -feature) %>% 
  ggplot(aes(feature, feature2, fill=value)) +
  geom_raster() +
  scale_fill_viridis_c() +
  theme(axis.text.x = element_text(angle=45, hjust=1)) +
  ggtitle("Concurvity plot\nFull Model - Sub 6")
```


* `mean_house_inc` and `pop_struggling_prop` still have high worst-case concurvity with each other - need to pick one

Try giving higher k to terms first - not sure if that will increase or decrease concurvity

```{r full_sub7}
full_model_sub7 <- gam(
    death_count_next_year ~ s(x,y) + s(year, k=3) + s(tot_pop, k = 12) + s(mean_house_inc, k = 12) + s(pop_struggling_prop, k =12) + town_status + s(opioid_rate_avg) + s(alprazolam_per_65_and_over, k = 12) + s(lorazepam_per_65_and_over),
    data = train_set, method = "REML", family = nb(), select = TRUE
    )
summary(full_model_sub7)
gam.check(full_model_sub7)
concurvity(full_model_sub7)
qq.gam(full_model_sub7, cex = 4)
# error on training
rmse(full_model_sub7, train_set)
# error on validation
rmse(full_model_sub7, valid_set)
rmse(full_model_sub6, valid_set)
```

Notes:

GAM Summary:
* alprazolam_per_65_and_over not significant

GAM check:
* although the k's have been increased, the p-values still indicate that more are needed - not sure if it's worth it (will likely lead to worse overfitting)

Concurvity:
* Worst-case concurvity metrics didn't change between sub 6 and this iteration 

RMSE Error:
* Validation error went down slightly


Likely not worth trying to change k at this point. Back pedal to previous iteration (sub 6) - check concurvities between mean income and population proportion that's struggling

```{r full_sub6_concurvity_ind}
full_model_sub6_conc %>% 
  select(feature, contains("mean")) %>% 
  arrange(desc(worst.s.mean_house_inc.))
full_model_sub6_conc %>% 
  select(feature, contains("pop_struggling_prop")) %>% 
  arrange(desc(worst.s.pop_struggling_prop.))
sum(full_model_sub6_conc$worst.s.mean_house_inc.)
sum(full_model_sub6_conc$worst.s.pop_struggling_prop.)
summary(full_model_sub6)
```

* The two features are pretty similar, not sure which to take out

Try sub 6, but take out alprazolam - does it make a diffrence?


```{r full_sub8}
full_model_sub8 <- gam(
    death_count_next_year ~ s(x,y) + s(year, k=3) + s(tot_pop) + s(mean_house_inc) + s(pop_struggling_prop) + town_status + s(opioid_rate_avg) + s(lorazepam_per_65_and_over),
    data = train_set, method = "REML", family = nb(), select = TRUE
    )
summary(full_model_sub8)
gam.check(full_model_sub8)
concurvity(full_model_sub8)
qq.gam(full_model_sub8, cex = 4)
# error on training
rmse(full_model_sub8, train_set)
# error on validation
rmse(full_model_sub8, valid_set)
rmse(full_model_sub6, valid_set)
```


Notes:

GAM Summary:
* All features now have a p-value < 0.05

GAM check:
* Not much has changed

Concurvity:
* Wost-case concurvity of `lorazepam_per_65_and_over` with the model has gone way down, but not surpsing since it had a strong relationship with alprazolam feature

RMSE Error:
* Validation error slightly better and concurvity between mean income and poverty metric didn't change much, maybe wait and see to remove alprazolam?


Try: sub 6 - but take out mean house inc - strong relationship with pop struggling and x,y:

```{r full_sub9}
full_model_sub9 <- gam(
    death_count_next_year ~ s(x,y) + s(year, k=3) + s(tot_pop) + s(pop_struggling_prop) + town_status + s(opioid_rate_avg) + s(alprazolam_per_65_and_over) + s(lorazepam_per_65_and_over),
    data = train_set, method = "REML", family = nb(), select = TRUE
    )
summary(full_model_sub9)
gam.check(full_model_sub9)
concurvity(full_model_sub9)
qq.gam(full_model_sub9, cex = 4)
# error on training
rmse(full_model_sub9, train_set)
# error on validation
rmse(full_model_sub9, valid_set)
rmse(full_model_sub6, valid_set)
```


Notes:

GAM Summary:
* n/A

GAM check:
* ignore

Concurvity:
* all features now have worst-case concurvity estimates below the desired threshold of 0.84

RMSE Error:
* Error on validation is much higher with this iteration than the previous one (sub6)


Try taking out poverty metric from model sub 6 instead? Will the validation error be lower with version sub 9 or sub 10?

```{r full_sub10}
full_model_sub10 <- gam(
    death_count_next_year ~ s(x,y) + s(year, k=3) + s(tot_pop) + s(mean_house_inc) + town_status + s(opioid_rate_avg) + s(alprazolam_per_65_and_over) + s(lorazepam_per_65_and_over),
    data = train_set, method = "REML", family = nb(), select = TRUE
    )
summary(full_model_sub10)
gam.check(full_model_sub10)
concurvity(full_model_sub10)
qq.gam(full_model_sub10, cex = 4)
# error on training
rmse(full_model_sub10, train_set)
# error on validation
rmse(full_model_sub10, valid_set)
rmse(full_model_sub6, valid_set)
```


Notes:

GAM Summary:
* alprazolam_per_65_and_over significant predictor with this model again too

GAM check:
* ignore

Concurvity:
* all concurvity metrics below desired threshold of 0.84 with this version also

RMSE Error:
* Validation error actually went down between previous iteration (sub6) and this one, much better than when the mean income was removed (sub9)


Last check - does removing the town shrunk/grown feature change things?

```{r}
full_model_sub10_alt <- gam(
    death_count_next_year ~ s(x,y) + s(year, k=3) + s(tot_pop) + s(mean_house_inc) + s(opioid_rate_avg) + s(alprazolam_per_65_and_over) + s(lorazepam_per_65_and_over),
    data = train_set, method = "REML", family = nb(), select = TRUE
    )
summary(full_model_sub10_alt)
gam.check(full_model_sub10_alt)
concurvity(full_model_sub10_alt)
qq.gam(full_model_sub10_alt, cex = 4)
# error on training
rmse(full_model_sub10_alt, train_set)
# error on validation
rmse(full_model_sub10, valid_set)
rmse(full_model_sub10_alt, valid_set)
```

A: no

- Features tentatively finalized


Try raising k again (because of gam check results), but on mod 10 - does it make a differnce? Improve fit?

```{r full_sub11}
full_model_sub11 <- gam(
    death_count_next_year ~ s(x,y, k = 45) + s(year, k=3) + s(tot_pop, k = 15) + s(mean_house_inc, k = 15) + town_status + s(opioid_rate_avg) + s(alprazolam_per_65_and_over, k = 15) + s(lorazepam_per_65_and_over),
    data = train_set, method = "REML", family = nb(), select = TRUE
    )
summary(full_model_sub11)
gam.check(full_model_sub11)
concurvity(full_model_sub11)
qq.gam(full_model_sub11, cex = 4)
# error on training
rmse(full_model_sub11, train_set)
# error on validation
rmse(full_model_sub11, valid_set)
rmse(full_model_sub10, valid_set)
```


Notes:

GAM Summary:
* All features still significant

GAM check:
* Increasing k doesn't seem to have done much - all features that had a p < 0.05 also have a p < 0.05 in this version. Is it worth it to raise k?

Concurvity:
* Worst-case concurvity estimates for `alprazolam_per_65_and_over` and  `lorazepam_per_65_and_over` are now at/above desired threshold of 0.84.

RMSE Error:
* Validation error slightly up

- Not worth it - likely increasing overfitting here with little to no benefit

Try rolling back k for x,y and mean house inc?

```{r full_sub12}
full_model_sub12 <- gam(
    death_count_next_year ~ s(x,y) + s(year, k=3) + s(tot_pop, k = 15) + s(mean_house_inc) + town_status + s(opioid_rate_avg) + s(alprazolam_per_65_and_over, k = 15) + s(lorazepam_per_65_and_over),
    data = train_set, method = "REML", family = nb(), select = TRUE
    )
summary(full_model_sub12)
gam.check(full_model_sub12)
concurvity(full_model_sub12)
qq.gam(full_model_sub12, cex = 4)
# error on training
rmse(full_model_sub12, train_set)
rmse(full_model_sub10, train_set)
# error on validation
rmse(full_model_sub12, valid_set)
rmse(full_model_sub11, valid_set)
rmse(full_model_sub10, valid_set)
```


Notes:

* Again, error doesn't change much and there doesn't seem to be a benefit to making the model more flexible by increasing k


Last sattempt to tweak k - try tuning up k for alprazolam and total population only (highest p-value)

```{r full_sub13}
full_model_sub13 <- gam(
    death_count_next_year ~ s(x,y) + s(year, k=3) + s(tot_pop) + s(mean_house_inc) + town_status + s(opioid_rate_avg) + s(alprazolam_per_65_and_over, k = 18) + s(lorazepam_per_65_and_over),
    data = train_set, method = "REML", family = nb(), select = TRUE
    )
summary(full_model_sub13)
gam.check(full_model_sub13)
concurvity(full_model_sub13)
qq.gam(full_model_sub13, cex = 4)
# error on training
rmse(full_model_sub13, train_set)
rmse(full_model_sub10, train_set)
# error on validation
rmse(full_model_sub13, valid_set)
rmse(full_model_sub12, valid_set)
rmse(full_model_sub11, valid_set)
rmse(full_model_sub10, valid_set)
```


Model is about the same - just go with model iteration 10 as the final formulation.


Retrain this formulation on the 2014-2017 data (2013-2016 here)

```{r final_model_train_eval}
train_full <- mod_df_count %>% 
  filter(year < 2017)
final_model <- gam(
    death_count_next_year ~ s(x,y) + s(year, k=3) + s(tot_pop) + s(mean_house_inc) + town_status + s(opioid_rate_avg) + s(alprazolam_per_65_and_over) + s(lorazepam_per_65_and_over),
    data = train_full, method = "REML", family = nb(), select = TRUE
    )
summary(final_model)
gam.check(final_model)
concurvity(final_model)
qq.gam(final_model, cex = 4)
# error on training
rmse(final_model, train_full)
# error on validation
rmse(full_model_sub10, test_set)
rmse(final_model, test_set)
mean(test_set$death_count_next_year)
sd(test_set$death_count_next_year)
```


Retraining gave a big boost to the predictive power - small difference between training and test set errors suggests that overfitting is not too big of a problem.


Final predictions:

```{r predictions}
test_set$fin_pred <- predict(final_model, test_set, type = "response")
sd(test_set$fin_pred)
mod_df_count$fin_pred <- predict(final_model, mod_df_count, type = "response")
```


## Model visualizations


```{r model_viz_qqplot_all_smooths}
library(mgcViz)
final_model_convert <- getViz(final_model)
qq(final_model_convert, rep = 10, method = "simul1", CI = "normal", showReps = TRUE,
        ngr = 1e2, a.replin = list(alpha = 0.1), a.qqpoi = list(shape = 19)) +
  ggtitle("QQ Plot\nNegative binomial GAM") +
  xlab("Theoretical Quantiles") +
  ylab("Deviance Residuals") +
  theme(
    axis.text.x = element_text(size = 20),
    axis.text.y = element_text(size = 20),
    axis.title = element_text(size = 20),
    plot.title = element_text(size = 20)
    )
# different variations on result plots
print(plot(final_model_convert, allTerms = T), pages = 1)
# directly save this from R plot interface to figures/tidy_figures/full_model_gam_plot.png
```


Final model QQ plot shows that most of the variation not captures by the model is at the extremes.

The second plot is a plot of all the feature smooths and their relationship with the smooth. With a GAM, coefficients are not directly interpretable (I think because they need to go through the link function to be converted to the expected outcome, and each feature will have different coefficients through the range of the value for each basis function). But the plot above can help explain the relationship of each individual feature with the target variable.


Testing other model plots, not too useful:

```{r model_viz_expreimentation}
# pick out individual term
plot(final_model_convert, allTerms = T, select = 7)
# does changing the residuals setting change anything?
print(plot(final_model_convert, allTerms = T, residuals = TRUE), pages = 1)
# no
# try with rug (represenin)
plot(final_model, select = 7, rug = TRUE)

# cleaned up version of one of the individual plots?
var_7 <- plot(sm(final_model_convert, 7))
var_7 + 
 # l_rug(mapping = aes(x=x, y=y), alpha = 0.8) +
  l_ciPoly() +
  l_fitLine(colour = "red") +
  #l_ciLine(mul = 5, colour = "blue", linetype = 2) + 
  l_points(shape = 19, size = 1, alpha = 0.1)
```

Can see some of the noisiness - need to be wary of interpreting result.

Would tidying up the odel to pull out coefficients be useful?

```{r model_tidy}
library(broom)
tidy(final_model)
tidy(final_model, parametric=TRUE)
coef(final_model)
```

Not sure what to do with it.

Save the final model df for easier plotting/mapping:

```{r mod_df_write}
#write_csv(mod_df_count, "../../../data/tidy_data/final_model_df_with_features_and_pred.csv")
```



